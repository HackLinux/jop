\label{sec:cache}


Worst-case execution time (WCET) analysis \cite{pusch:maxt:jnl} of
real-time programs is essential for any schedulability analysis. To
provide a low WCET value, a good processor model is necessary.
However, caches for the instructions and data is a classic example of
the paradigm \emph{Make the common case fast}, which complicates WCET
analysis. Avoiding or ignoring this feature in real-time systems, due
to its unpredictable behavior, results in a very pessimistic WCET
value. Plenty of effort has gone into research into integrating the
instruction cache in the timing analysis of tasks \cite{Arnold1994,
Healy1995, 225068} and the influence of the cache on task preemption
\cite{279589, Mataix:1996}. The influence of different cache
architectures on WCET analysis is described in
\cite{Heckmann:IEEE2003}.

We will tackle this problem from the architectural side -- an
instruction cache organization in which simpler and more accurate
WCET analysis is more important than average case performance.

In this section, we will explore the method cache, as it is
implemented in JOP, with a novel replacement policy. In Java bytecode
only relative branches exist, and a method is therefore only left
when a return instruction has been executed.\footnote{An uncaught
exception also results in a method exit.} It has been observed that
methods are typically short (see \cite{jop:thesis}) in Java
applications. These properties are utilized by a cache architecture
that stores complete methods. A complete method is loaded into the
cache on both invocation and return. This cache fill strategy lumps
all cache misses together and is very simple to analyze.

The method cache was first presented in \cite{jop:jtres_cache} and is
now also used by the Java processor SHAP \cite{shap:mcache}.
Furthermore, the idea has been adapted for a processor that executes
compiled C programs \cite{Metzlaff:SPM:2008}.

\subsection{Method Cache}

In this section, we will develop a solution for a time-predictable
instruction cache. Typical Java programs consist of short methods.
There are no branches out of the method and all branches inside are
relative. In the proposed architecture, the full code of a method is
loaded into the cache before execution. The cache is filled on
invocations and returns. This means that all cache fills are lumped
together with a known execution time. The full loaded method and
relative addressing inside a method also result in a simpler cache.
Tag memory and address translation are not necessary.

In the method cache several cache blocks (similar to cache lines) are
used for a method. The main difference from a conventional cache is
that the blocks for a method are all loaded at once and need to be
consecutive.

Choosing the block size is now a major design decision. Smaller
block sizes allow better memory usage, but the search time for a hit
also increases.

With varying block numbers per method, an LRU replacement becomes
impractical. When the method found to be LRU is smaller than the
loaded method, this new method invalidates two cached methods.

For the replacement, we will use a pointer $next$ that indicates the
start of the blocks to be replaced on a cache miss. Two practical
replace policies are:
%
\begin{description}
\item [Next block:]At the very first beginning, $next$ points to the
first block. When a method of length $l$ is loaded into the block
$n$, $next$ is updated to $(n+l)\;mod\;block\;count$.
\item [Stack oriented:]$next$ is updated in the same way as before
on a method load. It is also updated on a method return --
independent of a resulting hit or miss -- to point to the first
block of the leaving method.
\end{description}
%
We will show the operation of these different replacement policies
in an example with three methods: a(), b() and c() of block sizes 2,
2 and 1. The cache consists of 4 blocks and is therefore too small
to hold all the methods during the execution of the code fragment
shown in Listing~\ref{lst:cache:replace}.
%
\begin{samepage}
\begin{lstlisting}[float,caption={Code fragment for the replacement example},
label=lst:cache:replace]
    a() {
        for (;;) {
            b();
            c();
        }
        ...
    }
\end{lstlisting}
\end{samepage}
%
Tables \ref{tab_cache_replace_next} and
\ref{tab_cache_replace_stack} show the cache content during program
execution for both replacement policies. The content of the cache
blocks is shown after the execution of the invoke or return
instruction. An uppercase letter indicates that this block has been
newly loaded. A right arrow depicts the block to be replaced on a
cache miss (the \emph{next} pointer). The last row shows the number
of blocks that are filled during the execution of the program.

\begin{table}
    \centering
\begin{tt}
    \begin{tabular}{lrrrrrrrrrrr}
    \toprule

                &a()    &b()    &ret    &c()    &ret    &b()    &ret    &c()    &ret    &b()    &ret    \\
    \midrule
    \rm{Block 1}&A      &$\to$a &$\to$a &C      &c      &B      &b      &b      &$\to$- &B      &b      \\
    \rm{Block 2}&A      &a      &a      &$\to$- &A      &$\to$a &$\to$a &C      &c      &B      &b      \\
    \rm{Block 3}&$\to$- &B      &b      &b      &A      &a      &a      &$\to$- &A      &$\to$a &$\to$a \\
    \rm{Block 4}&-      &B      &b      &b      &$\to$- &B      &b      &b      &A      &a      &a      \\
    \midrule
    \rm{Fill}      &2   &4      &       &5      &7      &9      &       &11     &13     &15     &       \\
    \bottomrule

    \end{tabular}
\end{tt}
    \caption{Next block replacement policy}
    \label{tab_cache_replace_next}
\end{table}

\begin{table}
    \centering
\begin{tt}
    \begin{tabular}{lrrrrrrrrrrr}
    \toprule

                &a()    &b()    &ret    &c()    &ret    &b()    &ret    &c()    &ret    &b()    &ret    \\
    \midrule
    \rm{Block 1}&A      &$\to$a &a      &a      &a      &$\to$a &a      &a      &a      &$\to$a &a      \\
    \rm{Block 2}&A      &a      &a      &a      &a      &a      &a      &a      &a      &a      &a      \\
    \rm{Block 3}&$\to$- &B      &$\to$b &C      &$\to$c &B      &$\to$b &C      &$\to$c &B      &$\to$b \\
    \rm{Block 4}&-      &B      &b      &$\to$- &-      &B      &b      &$\to$- &-      &B      &b      \\
    \midrule
    \rm{Fill}      &2   &4      &       &5      &       &7      &       &8      &       &10     &       \\
    \bottomrule

    \end{tabular}
\end{tt}
    \caption{Stack oriented replacement policy}
    \label{tab_cache_replace_stack}
\end{table}

In this example, the stack oriented approach needs fewer fills, as
only methods b() and c() are exchanged and method a() stays in the
cache. However, if, for example, method b() is the size of one
block, all methods can be held in the cache using the the \emph{next
block} policy, but b() and c() would be still exchanged using the
\emph{stack} policy. Therefore, the first approach is used in the
proposed cache.


\subsection{WCET Analysis}

\label{sec:cache:wcet}

The proposed instruction cache is designed to simplify WCET
analysis. Due to the fact that all cache misses are only included in
two instructions (\emph{invoke} and \emph{return}), the instruction
cache can be ignored on all other instructions. The time needed to
load a complete method is calculated using the memory properties
(latency and bandwidth) and the length of the method. On an invoke,
the length of the invoked method is used, and on a return, the
method length of the caller is used to calculate the load time.

With a single method cache this calculation can be further
simplified. For every invoke there is a corresponding return. That
means that the time needed for the cache load on return can be
included in the time for the invoke instruction. This is simpler
because both methods, the caller and the callee, are known at the
occurrence of the invoke instruction. The information about which
method was the caller need not be stored for the return instruction
to be analyzed.

With more than one method in the cache, a cache hit detection has to
be performed as part of the WCET analysis. If there are only two
blocks, this is trivial, as (i) a hit on invoke is only possible if
the method is the same as the last invoked (e.g.\ a single method in
a loop) and (ii) a hit on return is only possible when the method is
a leaf in the call tree. In the latter case, it is always a hit.

When the cache contains more blocks (i.e.\ more than two methods can
be cached), a part of the call tree has to be taken into account for
hit detection. The method cache further complicates the analysis, as
the method length also determines the cache content. However, this
analysis is still simpler than a cache modeling of a direct-mapped
instruction cache, as cache block replacement depends on the call
tree instead of instruction addresses.

In traditional caches, data access and instruction cache fill
requests can compete for the main memory bus. For example, a load or
store at the end of the processor pipeline competes with an
instruction fetch that results in a cache miss. One of the two
instructions is stalled for additional cycles by the other
instruction. With a data cache, this situation can be even worse.
The worst-case scenario for the memory stall time for an instruction
fetch or a data load is two miss penalties when both cache reads are
a miss. This unpredictable behavior leads to very pessimistic WCET
bounds.

A \emph{method cache}, with cache fills only on invoke and return,
does not interfere with data access to the main memory. Data in the
main memory is accessed with \emph{getfield} and \emph{putfield},
instructions that never overlap with \emph{invoke} and
\emph{return}. This property removes another uncertainty found in
traditional cache designs.


\subsection{Caches Compared}

In this section, we will compare the different cache architectures
in a quantitative way. Although our primary concern is
predictability, performance remains important. We will therefore
first present the results from a conventional direct-mapped
instruction cache. These measurements will then provide a baseline
for the evaluation of the proposed architecture.

Cache performance varies with different application domains. As the
proposed system is intended for real-time applications, the
benchmark for these tests should reflect this fact. However, there
are no standard benchmarks available for embedded real-time systems.
A real-time application was therefore adapted to create this
benchmark. The application is from one node of a distributed motor
control system \cite{jop:wises03} (see also
Section~\ref{sec:app:kfl}). A simulation of the environment (sensors
and actors) and the communication system (commands from the master
station) forms part of the benchmark for simulating the real-world
workload.

The data for all measurements was captured using a simulation of JOP
and running the application for 500,000 clock cycles. During this
time, the major loop of the application was executed several hundred
times, effectively rendering any misses during the initialization
code irrelevant to the measurements.

\subsubsection{Direct-Mapped Cache}

\tablename~\ref{tab_cache_direct} gives the memory bytes and memory
transactions per instruction byte for a standard direct-mapped
cache. As we can see from the values for a cache size of 4KB, the
kernel of the application is small enough to fit completely into the
4KB cache. The cache performs better (i.e.\ fewer bytes are
transferred) with smaller block sizes. With smaller block sizes, the
chance of unused data being read is reduced and the larger number of
blocks reduces conflict misses. However, reducing the block size
also increases memory transactions (MTIB), which directly relates to
memory latency.


\begin{table}
    \centering
    \begin{tabular}{cd{2.0}cc}
    \toprule

    Cache size & \cc{Block size} & MBIB & MTIB \\

    \midrule

     1 KB &  8 & 0.28 & 0.035 \\
     1 KB & 16 & 0.38 & 0.024 \\
     1 KB & 32 & 0.58 & 0.018 \\
     2 KB &  8 & 0.17 & 0.022 \\
     2 KB & 16 & 0.25 & 0.015 \\
     2 KB & 32 & 0.41 & 0.013 \\
     4 KB &  8 & 0.00 & 0.001 \\
     4 KB & 16 & 0.01 & 0.000 \\
     4 KB & 32 & 0.01 & 0.000 \\

    \bottomrule

    \end{tabular}
    \caption{Direct-mapped cache}
    \label{tab_cache_direct}
\end{table}

Which configuration performs best depends on the relationship between
memory bandwidth and memory latency. Examples of average memory
access times in cycles per instruction byte for different memory
technologies are provided in Table~\ref{tab_cache_direct_mem}. The
third column shows the cache performance for a Static RAM (SRAM) that
is very common in embedded systems. A latency of 1 clock cycle and an
access time of 2 clock cycles per 32-bit word are assumed. For the
synchronous DRAM (SDRAM) in the forth column, a latency of 5 cycles
(3 cycles for the row address and 2 cycles for the CAS latency) is
assumed. The memory delivers one word (4 bytes) per cycle. The Double
Data Rate (DDR) SDRAM in the last column has an enhanced latency of
4.5 cycles and transfers data on both the rising and falling edge of
the clock signal.

%
%       simulate 32-bits (DDR) SDRAM
%
%       SDRAM:  5 cycle latency: 3 cycle row address and 2 cycle CAS latency
%               4 Bytes / cycle (0.25 / Byte)
%
%       DDR:    4.5 cycle latency: 2 cycle row address and 2.5 cycle CAS latency
%               4 Bytes / 0.5 cycle (0.125 / Byte)
%
%       SRAM 15 ns at 100 MHz: 1 cycle latency, 2 cycles / word
%


\begin{table}
    \centering
    \begin{tabular}{cd{2.0}ccc}
    \toprule

    Cache size & \cc{Block size} & SRAM & SDRAM & DDR \\

    \midrule

    1 KB & 8 & \textbf{0.18} & 0.25 & 0.19 \\
    1 KB & 16 & 0.22 & \textbf{0.22} & 0.16 \\
    1 KB & 32 & 0.31 & 0.24 & \textbf{0.15} \\
    2 KB & 8 & \textbf{0.11} & 0.15 & 0.12 \\
    2 KB & 16 & 0.14 & \textbf{0.14} & \textbf{0.10} \\
    2 KB & 32 & 0.22 & 0.17 & 0.11 \\

    \bottomrule

    \end{tabular}
    \caption{Direct-mapped cache, average memory access time}
    \label{tab_cache_direct_mem}
\end{table}

The data in bold give the best block size for different memory
technologies. As expected, memories with a higher latency and
bandwidth perform better with larger block sizes. For small block
sizes, the latency clearly dominates the access time. Although the
SRAM has half the bandwidth of the SDRAM and a quarter of the DDR,
with a block size of 8 bytes, it is faster than the DRAM memories.
In most cases a block size of 16 bytes is the fastest solution and
we will therefore use this configuration for comparison with the
following cache solutions.

\subsubsection{Fixed Block Cache}

Cache performance for single method per block architectures is shown
in Table~\ref{tab_cache_block}. The measurements for a simple 8 byte
prefetch queue are also given, for reference. With prefetching, we
would expect to see an MBIB of about 1. The 37\% overhead results
from the fact that the prefetch queue fetches 4 bytes a time and has
to buffer a minimum of 3 bytes for the instruction fetch stage. On a
branch or return, the queue is flushed and these bytes are lost.

A single block that has to be filled on every invoke and return
requires considerable overheads. More than twice the amount of data
is read from the main memory than is consumed by the processor.
However, the memory transaction count is 16 times lower than with
simple prefetching, which can compensate for the large MBIB for main
memories with high latency.

The solution with two blocks for two methods performs almost twice
as well as the simple one method cache. This is due to the fact
that, for all leaves in the call tree, the caller method can be
found on return. If the block count is doubled again, the number of
misses is reduced by a further 25\%, but the cache size also
doubles. For this measurement, an LRU replacement policy applies for
the two and four block caches.

\begin{table}
    \centering
    \begin{tabular}{lcccc}
    \toprule

    Type & Cache size & MBIB & MTIB \\

    \midrule
        Prefetch & 8 B & 1.37 & 0.342  \\
        Single method & 1 KB & 2.32 & 0.021  \\
        Two blocks & 2 KB & 1.21 & 0.013  \\
        Four blocks & 4 KB & 0.90 & 0.010  \\
    \bottomrule

    \end{tabular}
    \caption{Fixed block cache}
    \label{tab_cache_block}
\end{table}

The same memory parameters as in the previous section are also used
in Table~\ref{tab_cache_block_mem}. With the high latency of the
DRAMs, even the simple one block cache is a faster (and more
accurately predictable) solution than a prefetch queue. As MBIB and
MTBI show the same trend as a function of the number of blocks, this
is reflected in the access time in all three memory examples.

\begin{table}
    \centering
    \begin{tabular}{lcccc}
    \toprule

    Type & Cache size & SRAM & SDRAM & DDR \\

    \midrule
        Prefetch & 8 B & 1.02 & 2.05 & 1.71 \\
        Single Method & 1 KB & 1.18 & 0.69 & 0.39 \\
        Two blocks & 2 KB & 0.62 & 0.37 & 0.21 \\
        Four blocks & 4 KB & 0.46 & 0.27 & 0.16 \\
    \bottomrule

    \end{tabular}
    \caption{Fixed block cache, average memory access time}
    \label{tab_cache_block_mem}
\end{table}

\subsubsection{Variable Block Cache}

\tablename~\ref{tab_cache_var_block} shows the cache performance of
the proposed solution, i.e.\ of a method cache with several blocks
per method, for different cache sizes and number of blocks. For this
measurement, a \emph{next block} replacement policy applies.

\begin{table}
    \centering
    \begin{tabular}{cd{2.0}cc}
    \toprule

    Cache size & \cc{Block count} & MBIB & MTIB \\

    \midrule
        1 KB & 8 & 0.80 & 0.009  \\
        1 KB & 16 & 0.71 & 0.008  \\
        1 KB & 32 & 0.70 & 0.008  \\
        1 KB & 64 & 0.70 & 0.008  \\
        2 KB & 8 & 0.73 & 0.008  \\
        2 KB & 16 & 0.37 & 0.004  \\
        2 KB & 32 & 0.24 & 0.003  \\
        2 KB & 64 & 0.12 & 0.001  \\
        4 KB & 8 & 0.73 & 0.008  \\
        4 KB & 16 & 0.25 & 0.003  \\
        4 KB & 32 & 0.01 & 0.000  \\
        4 KB & 64 & 0.00 & 0.000  \\
    \bottomrule

    \end{tabular}
    \caption{Variable block cache}
    \label{tab_cache_var_block}
\end{table}

In this scenario, as the MBIB is very high at a cache size of 1KB
and almost independent of the block count, the cache capacity is
seen to be clearly dominant. The most interesting cache size with
this benchmark is 2KB. Here, we can see the influence of the number
of blocks on both performance parameters. Both values benefit from
more blocks. However, a higher block count requires more time or
more hardware for the hit detection. With a cache size of 4KB and
enough blocks, the kernel of the application completely fits into
the variable block cache, as we have seen with a 4KB traditional
cache. From the gap between 16 and 32 blocks (within the 4KB cache),
we can say that the application consists of fewer than 32 different
methods.

It can be seen that even the smallest configuration with a cache
size of 1KB and only 8 blocks outperforms fixed block caches with 2
or 4KB in both parameters (MBIB and MTIB). Compared with the fixed
block solutions, MTIB is low in all configurations. This is due to
the better hit rate, as indicated by the lower MBIB.

In most configurations, MBIB is higher than for the direct-mapped
cache. It is very interesting to note that, in all configurations
(even the small 1KB cache), MTIB is lower than in all 1KB and 2KB
configurations of the direct-mapped cache. This is a result of the
complete method transfers when a miss occurs and is clearly an
advantage for main memory systems with high latency.

As in the previous examples, Table~\ref{tab_cache_var_block_mem}
shows the average memory access time per instruction byte for three
different main memories.

\begin{table}
    \centering
    \begin{tabular}{cd{2.0}ccc}
    \toprule

    Cache size & \cc{Block count} & SRAM & SDRAM & DDR \\

    \midrule
        1 KB & 8 & 0.41 & 0.24 & 0.14 \\
        1 KB & 16 & 0.36 & 0.22 & 0.12 \\
        1 KB & 32 & 0.36 & 0.21 & 0.12 \\
        1 KB & 64 & 0.36 & 0.21 & 0.12 \\
        2 KB & 8 & 0.37 & 0.22 & 0.13 \\
        2 KB & 16 & 0.19 & 0.11 & 0.06 \\
        2 KB & 32 & 0.12 & 0.08 & 0.04 \\
        2 KB & 64 & 0.06 & 0.04 & 0.02 \\
    \bottomrule

    \end{tabular}
    \caption{Variable block cache, average memory access time}
    \label{tab_cache_var_block_mem}
\end{table}

In the DRAM configurations, the variable block cache directly
benefits from the low MTBI. When comparing the values between SDRAM
and DDR, we can see that the bandwidth affects the memory access
time in a way that is approximately linear. The high latency of
these memories is completely hidden. The configuration with 16 or
more blocks and dynamic RAMs outperforms the direct-mapped cache of
the same size. As expected, a memory with low latency (the SRAM in
this example) depends on the MBIB values. The variable block cache
is slower than the direct-mapped cache in the 1KB configuration
because of the higher MBIB (0.7 compared to 0.3-0.6), and performs
very similarly at a cache size of 2KB.

In Table~\ref{tab_cache_compared}, the different cache solutions with
a size of 2KB are summarized.
%
\begin{table}
    \centering
    \begin{tabular}{lcc}
    \toprule

    Cache type & MBIB & MTIB \\

    \midrule
        Single method        & 2.32 & 0.021  \\
        Two blocks           & 1.21 & 0.013  \\
        Variable block (16)  & 0.37 & 0.004  \\
        Variable block (32)  & 0.24 & 0.003  \\
        Direct-mapped        & 0.25 & 0.015 \\
    \bottomrule

    \end{tabular}
    \caption{Caches compared}
    \label{tab_cache_compared}
\end{table}
%
All full method caches with two or more blocks have a lower MTIB
than a conventional cache solution. This becomes more significant
with increasing latency in main memories. The MBIB value is only
quite high for one or two methods in the cache. However, the most
surprising result is that the variable block cache with 32 blocks
outperforms a direct-mapped cache of the same size at both values.

We can see that predictability is indirectly related to performance
-- a trend we had anticipated. The most predictable solution with a
single method cache performs very poorly compared to a conventional
direct-mapped cache. If we accept a slightly more complex WCET
analysis (taking a small part of the call tree into account), we can
use the two-block cache that is about two times better.

With the variable block cache, it could be argued that the WCET
analysis becomes too complex, but it is nevertheless simpler than
that with the direct-mapped cache. However, every hit in the
two-block cache will also be a hit in a variable block cache (of the
same size). A tradeoff might be to analyze the program by assuming a
two-block cache but using a version of the variable block cache. The
additional performance gain can than be used by non- or soft
real-time parts of an application.


\subsection{Summary}

In this section, we have extended the single cache performance
measurement \emph{miss rate} to a two value set, memory read and
transaction rate, in order to perform a more detailed evaluation of
different cache architectures. From the properties of the Java
language -- usually small methods and relative branches -- we
derived the novel idea of a \emph{method cache}, i.e.\ a cache
organization in which whole methods are loaded into the cache on
method invocation and the return from a method. This cache
organization is time-predictable, as all cache misses are lumped
together in these two instructions. Using only one block for a
single method introduces considerable overheads in comparison with a
conventional cache, but is very simple to analyze. We extended this
cache to hold more methods, with one block per method and several
smaller blocks per method.

Comparing these organizations quantitatively with a benchmark
derived from a real-time application, we have seen that the
variable block cache performs similarly to (and in one configuration
even better than) a direct-mapped cache, in respect of the bytes
that have to be filled on a cache miss. In all configurations and
sizes of the variable block cache, the number of memory
transactions, which relates to memory latency, is lower than in a
traditional cache.

Only filling the cache on method invocation and return simplifies
WCET analysis and removes another source of uncertainty, as there is
no competition for the main memory access between instruction cache
and data cache.

%\subsubsection{Future Work}
%\begin{itemize}
%    \item Variation of direct-mapped with link addresses
%    Varies between version of paper and now
%    \item cache graphs: cycles per loop
%    \item more benchmarks
%\end{itemize}

%\bibliographystyle{plain}
%\bibliography{../bib/mybib}
%
%\begin{table*}
%    \centering
%    \begin{tabular}{lcccccc}
%    \toprule
%    & & & & \multicolumn{3}{c}{Memory access time} \\
%    \cmidrule{5-7}
%    Type & Size & MBIB & MTIB & SRAM & SDRAM & DDR SDRAM \\
%    \midrule
%
%    Prefetch buffer & 8 B & 1.37 & 0.342                 & 1.02 & 2.05 & 1.71 \\
%    Single method cache & 1 KB & 2.32 & 0.021            & 1.18 & 0.69 & 0.39 \\
%    Two block cache & 2 KB & 1.21 & 0.013                & 0.62 & 0.37 & 0.21 \\
%    Four block cache & 4 KB & 0.90 & 0.010               & 0.46 & 0.27 & 0.16 \\
%    Direct mapped 8 bytes & 1 KB & 0.28 & 0.035          & 0.18 & 0.25 & 0.19 \\
%    Direct mapped 16 bytes & 1 KB & 0.38 & 0.024         & 0.22 & 0.22 & 0.16 \\
%    Direct mapped 32 bytes & 1 KB & 0.58 & 0.018         & 0.31 & 0.24 & 0.15 \\
%    Direct mapped 8 bytes & 2 KB & 0.17 & 0.022          & 0.11 & 0.15 & 0.12 \\
%    Direct mapped 16 bytes & 2 KB & 0.25 & 0.015         & 0.14 & 0.14 & 0.10 \\
%    Direct mapped 32 bytes & 2 KB & 0.41 & 0.013         & 0.22 & 0.17 & 0.11 \\
%    Direct mapped 8 bytes & 4 KB & 0.00 & 0.001          & 0.00 & 0.00 & 0.00 \\
%    Direct mapped 16 bytes & 4 KB & 0.01 & 0.000         & 0.00 & 0.00 & 0.00 \\
%    Direct mapped 32 bytes & 4 KB & 0.01 & 0.000         & 0.00 & 0.00 & 0.00 \\
%    Variable block cache 8 blocks & 1 KB & 0.80 & 0.009  & 0.41 & 0.24 & 0.14 \\
%    Variable block cache 16 blocks & 1 KB & 0.71 & 0.008 & 0.36 & 0.22 & 0.12 \\
%    Variable block cache 32 blocks & 1 KB & 0.70 & 0.008 & 0.36 & 0.21 & 0.12 \\
%    Variable block cache 64 blocks & 1 KB & 0.70 & 0.008 & 0.36 & 0.21 & 0.12 \\
%    Variable block cache 8 blocks & 2 KB & 0.73 & 0.008  & 0.37 & 0.22 & 0.13 \\
%    Variable block cache 16 blocks & 2 KB & 0.37 & 0.004 & 0.19 & 0.11 & 0.06 \\
%    Variable block cache 32 blocks & 2 KB & 0.24 & 0.003 & 0.12 & 0.08 & 0.04 \\
%    Variable block cache 64 blocks & 2 KB & 0.12 & 0.001 & 0.06 & 0.04 & 0.02 \\
%    Variable block cache 8 blocks & 4 KB & 0.73 & 0.008  & 0.37 & 0.22 & 0.13 \\
%    Variable block cache 16 blocks & 4 KB & 0.25 & 0.003 & 0.13 & 0.08 & 0.05 \\
%    Variable block cache 32 blocks & 4 KB & 0.01 & 0.000 & 0.00 & 0.00 & 0.00 \\
%    Variable block cache 64 blocks & 4 KB & 0.00 & 0.000 & 0.00 & 0.00 & 0.00 \\
%
%    \bottomrule
%
%    \end{tabular}
%    \caption[Cache performance compared]{Cache performance in MBIB and MTIB of all variations of
%    the method cache and a conventional direct mapped cache. Average
%    memory access time per instruction byte for three different main
%    memory technologies. All numbers are in clock cycles.
%    }
%    \label{tab_cache_all}
%\end{table*}
%
%
%
%\end{document}
